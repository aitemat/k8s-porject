# Tổng quan
## iSCSI là gì?
`Persistent Volume (PV)` trong Kubernetes là một tài nguyên cấp cao, đại diện cho một phần trữ lượng lưu trữ bên ngoài được cung cấp cho các container trong cluster. PV giúp cho việc quản lý và sử dụng các tài nguyên lưu trữ trở nên dễ dàng và linh hoạt hơn.

`iSCSI` (Internet Small Computer System Interface) là một giao thức lưu trữ cho phép máy chủ kết nối đến các thiết bị lưu trữ thông qua mạng IP. Trong Kubernetes, iSCSI được sử dụng để kết nối đến các thiết bị lưu trữ bên ngoài, chia sẻ các tài nguyên lưu trữ đó với các container trong cluster.

## Một số thuật ngữ

+ `iSCSI Target Server`: Server quản lý các thiết bị lưu trữ.

+ `Physical Disk`: Đĩa vật lý. Thông thường 1 iSCSI Target Server phải gắn nhiều đĩa vật lý.

+ `iSCSI Virtual Disk`: Là các đĩa ảo được tạo ra từ 1 Physical Disk.

+ `iSCSI Target`: Là tập hợp gồm 1 hoặc nhiều iSCSI Virtual Disk để các Server có thể kết nối đến và sử dụng.

+ `iSCSI Initiator (Access Server)`: là 1 Server đảm nhận một dịch vụ bất kỳ mà nó sẽ kết nối tới iSCSI Target để sử dụng hệ thống đĩa.

+ `Storage Pool`: là tập hợp các iSCSI Virtual Disk đã kết nối được từ iSCSI Target mà chúng ta muốn sử dụng.

+ `Virtual Disk`: là 1 đĩa cứng ảo, dùng để xác định mục đích sử dụng.

+ `Volume`: là ổ đĩa luận lý được gán ký tự đĩa và được sử dụng để lưu trữ dữ liệu cho server
## Các bước triển khai cài đặt iSCSI
- Cài đặt iSCSI Target Server

- Tạo iSCSI Virtual Disk

- Kết nối đến iSCSI Target

- Tạo Storage Pool

- Tạo Virtual Disk

- Tạo Volume

- Kiểm tra
# 1. Requirement
`Master 1`: Docker + Master node (RAM >=3GB + CPU 4 + Disk >=30GB: 45.117.80.52 - Ubuntu 20.04 - hostname: k8s-master1  
`Master 2`: Docker + Master node (RAM >=3GB + CPU 4 + Disk >=30GB: 103.101.162.200 - Ubuntu 20.04 - hostname: k8s-master2  
`Master 3`: Haproxy(RAM >=3GB + CPU 4 + Disk >=30GB: 45.117.80.150 - Ubuntu 20.04 - hostname: k8s-master3- Haproxy  
`iSCSI Server`: Window server 2022 (RAM >=3GB + CPU 4 + Disk >=40GB): 103.101.162.145 - iSCSI server   
`Node 1`: Docker + Node worker 1 ( RAM 2GB + CPU 1 + Disk 20GB): 103.101.163.72 - Ubuntu 20.04 - hostname: k8s-nodew1  
`Node 2`: Docker + Node worker 2 ( RAM 2GB + CPU 1 + Disk 20GB): 103.101.162.20 - Ubuntu 20.04 - hostname: k8s-nodew1  

# 2. Cài đặt và cấu hình iSCSI target trên window server 2022 (iSCSI Server)
## 2.1. Cài đặt iSCSI target
**B1:** Mở Server Manager, vào menu Manage, chọn Add Roles and Features.  
<img src="/images/iscsi1.jpg">  
**B2:** Cửa sổ Before You Begin, chọn Next 3 lần.  
**B3:** Cửa sổ Select server roles, bung mục File and Storage services, đánh dấu chọn iSCSI Target Server, chọn Add Features, sau đó chọn Next 2 lần  

<img src="/images/iscsi2.jpg"> 

**B4:** Cửa sổ Confirm installation selections, chọn Install, chọn Close.  

<img src="/images/iscsi3.jpg">  

<img src="/images/iscsi4.jpg"> 

## 2.2. Cấu hình iSCSI
B1 - Vào Server Manager, chọn `File and Storage Services` ở khung bên trái. Tiếp theo chọn iSCSI, chọn liên kết `start the New iSCSI Virtual Disk Wizard`.  

<img src="/images/iscsi5.jpg"> 

B2 - Cửa sổ Select `iSCSI virtual disk location`, ở mục `Select by volume`, chọn vào phân vùng cần tạo `iSCSI Virtual Disk` (trong bài lab này sẽ tạo Disk1 từ ổ đĩa C), chọn Next.  

<img src="/images/iscsi6.jpg"> 

B3 - Cửa sổ `Specify iSCSI virtual disk name`, nhập `disk-share1` vào ô Name và chọn Next.  

<img src="/images/iscsi7.jpg"> 

B4 – Cửa sổ `Specify iSCSI virtual disk size`, nhập 10 GB vào ô Size, và chọn Next.

<img src="/images/iscsi8.jpg">

B5 – Cửa sổ `Assign iSCSI target`, chọn `New iSCSI target`, và chọn Next.  

<img src="/images/iscsi9.jpg"> 

B6 – Cửa sổ `Specify target name`, nhập target1 vào ô Name, và chọn Next.  

<img src="/images/iscsi10.jpg"> 

B7 – Cửa sổ `Specify access servers`, chọn Add để thêm vào máy Server sẽ kết nối đến iSCSI Target nhằm mục đích sử dụng hệ thống đĩa.  
B8 – Cửa sổ `Add initiator ID`, chọn Enter `a value for the selected type`, bung ô Type, chọn `IP Address`, nhập IP tất cả các node cần sử dụng trong mô hình vào ô Value và chọn OK. 

<img src="/images/iscsi11.jpg"> 

B9 – Cửa sổ `Specify access servers`, chọn Next.  
B10 – Cửa sổ Enable Authentication, chọn Next (trong lab này sẽ chọn bỏ qua authen)

<img src="/images/iscsi12.jpg"> 

B11 – Cửa sổ `Confirm Selections`, chọn Create.  

<img src="/images/iscsi13.jpg"> 

B12 – Cửa sổ `View Results`, chọn Close.  

<img src="/images/iscsi14.jpg"> 

# 2.3. Kết nối qua iSCSI Initiator
Trong mô hình này ta kết nối tới tất cả node master và worker trong cụm.
## Cài đặt gói iSCSI Initiator
`apt-get update -y`  
`sudo apt-get install open-iscsi`  
## Cấu hình kết nối
Sử dụng lệnh sau để tìm iSCSI target trên IP:
`sudo iscsiadm -m discovery -t st -p 103.101.162.145`  
Trong đó:  
- `103.101.162.145`: là IP iSCSI Target server  

Khi bạn nhập lệnh trên, iSCSI Initiator sẽ tìm kiếm iSCSI target trên máy chủ iSCSI và hiển thị danh sách các iSCSI target đã được tìm thấy.  
Chạy 3 lệnh sau trên các node worker và master:  
`sudo iscsiadm -m node -o new -T iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target -p 103.101.162.145`  
Lệnh này tạo một kết nối mới đến iSCSI target có địa chỉ IP là 103.101.162.145 và tên là iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target  
`sudo iscsiadm -m node -o update -T iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target -p 103.101.162.145 -n node.startup -v automatic` 
Lệnh này cập nhật cấu hình của iSCSI target đã được tạo trước đó. Nó đặt giá trị của thuộc tính node.startup thành automatic, có nghĩa là iSCSI target sẽ được tự động kết nối khi máy chủ khởi động.  
`sudo iscsiadm -m node -T iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target -p 103.101.162.145 -l` 
Lệnh này kết nối đến iSCSI target đã được tạo trước đó bằng cách sử dụng địa chỉ IP và tên của nó. Khi kết nối thành công, iSCSI target sẽ được hiển thị như một thiết bị lưu trữ trên máy chủ.  

# 3. Triển khai app trên Master1 
## Random số lượng pod chia đều cho các node worker
Viết 1 file `deploy_iscsi.yaml` :
```sh
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-10g
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  iscsi:
    targetPortal: 103.101.162.145:3260  # IP port iSCSI server
    iqn: iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target  # iSCSI target đã triển khai trên window server 2022
    lun: 0
    fsType: ext4
    readOnly: false
  persistentVolumeReclaimPolicy: Retain

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-10g
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: iscsi-volume
          mountPath: /usr/share/nginx/html   # thư mục web của service nginx
      volumes:
      - name: iscsi-volume
      volumes:
      - name: iscsi-volume
        persistentVolumeClaim:
          claimName: pvc-10g  # pod sử dụng storage của pvc-10g

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  ports:
    - name: nginx
      nodePort: 30070  # port public đễ bên ngoài truy cập
      port: 80
      protocol: TCP
      targetPort: 80
  selector:
    app: nginx

```
Chạy lệnh trên đễ tạo PV, PVC, deployment, pod và public service qua nodeport: `kubectl apply -f deploy_iscsi.yaml`  

<img src="/images/iscsi15.jpg">  

Trong đó ta thấy khi sử dụng replicas=2 thì pod sẽ chia đều cho 2 node worker: 

<img src="/images/iscsi16.jpg">  

<img src="/images/iscsi17.jpg">  

## Gán cố định pod cho 1 node worker
Viết 1 file `deploy_iscsi.yaml` :
```sh
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-10g
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  iscsi:
    targetPortal: 103.101.162.145:3260
    iqn: iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target
    lun: 0
    fsType: ext4
    readOnly: false
  persistentVolumeReclaimPolicy: Retain

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-10g
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  selector:
    matchLabels:
      node: k8s-nodew1  #Chọn node cố định phù hợp với pvc
  storageClassName: ""
  volumeMode: Filesystem
  volumeName: pv-10g

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: iscsi-volume
          mountPath: /usr/share/nginx/html
      nodeSelector:
        kubernetes.io/hostname: k8s-nodew1  #Chọn cố định pod sẽ triển khai
      volumes:
      - name: iscsi-volume
        persistentVolumeClaim:
          claimName: pvc-10g

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  ports:
    - name: nginx
      nodePort: 30070
      port: 80
      protocol: TCP
      targetPort: 80
  selector:
    app: nginx

```

Triển khai bằng lệnh sau: `kubectl apply -f deploy_iscsi.yaml`  
Sau khi 2 pod được tạo thành ta thấy 2 pod trên đều triển khai trên `k8s-nodew1`  

<img src="/images/iscsi18.jpg"> 

<img src="/images/iscsi19.jpg">

# 4. Kiểm tra va ứng dụng
## 4.1 Test delete app xong deploy lại xem dữ liệu còn không.
```sh
root@k8s-master1:~# kubectl delete -f deploy_iscsi.yaml
persistentvolume "pv-10g" deleted
persistentvolumeclaim "pvc-10g" deleted
deployment.apps "nginx" deleted
service "nginx-service" deleted
root@k8s-master1:~# kubectl apply -f deploy_iscsi.yaml
persistentvolume/pv-10g created
persistentvolumeclaim/pvc-10g created
deployment.apps/nginx created
service/nginx-service created
root@k8s-master1:~# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-8645766dbb-fvfvw   0/1     Pending   0          8s
nginx-8645766dbb-vxcd7   0/1     Pending   0          8s
root@k8s-master1:~# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-8645766dbb-fvfvw   1/1     Running   0          30s
nginx-8645766dbb-vxcd7   1/1     Running   0          30s
root@k8s-master1:~# kubectl get services
NAME            TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes      ClusterIP      10.96.0.1        <none>        443/TCP        9d
nginx-nhanhoa   LoadBalancer   10.108.234.255   <pending>     80:31023/TCP   5d23h
nginx-service   NodePort       10.101.140.127   <none>        80:30070/TCP   92s
root@k8s-master1:~#
```
Kết quả sau khi xóa và deploy lại theo cấu hình file cũ, thì dữ liệu vẫn còn dữ liệu mà không mất đi.
## 4.2 Kiểm tra pod có Persitent Volume (PV) đang kết nối qua iSCSI target không?
```sh
root@k8s-master1:~# kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   REASON   AGE
pv-10g   10Gi       RWX            Retain           Bound    default/pvc-10g                           139m
root@k8s-master1:~# kubectl describe pv pv-10g
Name:            pv-10g
Labels:          <none>
Annotations:     pv.kubernetes.io/bound-by-controller: yes
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:
Status:          Bound
Claim:           default/pvc-10g
Reclaim Policy:  Retain
Access Modes:    RWX
VolumeMode:      Filesystem
Capacity:        10Gi
Node Affinity:   <none>
Message:
Source:
    Type:               ISCSI (an ISCSI Disk resource that is attached to a kubelet's host machine and then exposed to the pod)
    TargetPortal:       103.101.162.145:3260
    IQN:                iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target  #kiểm tra nếu có dòng này và dòng trên thì PV đã kết nối thành công
    Lun:                0
    ISCSIInterface      default
    FSType:             ext4
    ReadOnly:           false
    Portals:            []
    DiscoveryCHAPAuth:  false
    SessionCHAPAuth:    false
    SecretRef:          nil
    InitiatorName:      <none>
Events:                 <none>
```

## 4.3 Dữ liệu sẽ được lưu ở đâu
Kiểm tra bằng cách sử dụng kệnh `kubectl describe pod <tên pod>` đễ kiểm tra pod dăng hoạt động trên node nào.  
```sh
root@k8s-master1:~# kubectl get pod
NAME                     READY   STATUS    RESTARTS   AGE
nginx-8645766dbb-967r9   1/1     Running   0          5h8m
nginx-8645766dbb-prph5   1/1     Running   0          5h8m
root@k8s-master1:~# kubectl describe pod nginx-8645766dbb-967r9
Name:             nginx-8645766dbb-967r9
Namespace:        default
Priority:         0
Service Account:  default
Node:             k8s-nodew2/103.101.162.20   # vị trí node worker được gán
Start Time:       Wed, 26 Apr 2023 16:44:22 +0700
Labels:           app=nginx
                  pod-template-hash=8645766dbb
Annotations:      <none>
Status:           Running
IP:               10.244.3.45
IPs:
  IP:           10.244.3.45
Controlled By:  ReplicaSet/nginx-8645766dbb
Containers:
  nginx:
    Container ID:   containerd://2933922e0487617d42b58daab68e6891f0215289a643b8f7291c5f2138114793
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:63b44e8ddb83d5dd8020327c1f40436e37a6fffd3ef2498a6204df23be6e7e94
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Wed, 26 Apr 2023 16:44:33 +0700
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /usr/share/nginx/html from iscsi-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gf97w (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  iscsi-volume:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  pvc-10g
    ReadOnly:   false
  kube-api-access-gf97w:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>
```
Sau đó vào node `k8s-nodew2` show `df -h` đễ kiểm tra tra tên pv  
```sh
root@k8s-nodew2:/# df -h
Filesystem      Size  Used Avail Use% Mounted on
udev            1.9G     0  1.9G   0% /dev
tmpfs           394M  1.5M  392M   1% /run
/dev/vda2        30G  5.8G   23G  21% /
tmpfs           2.0G     0  2.0G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           2.0G     0  2.0G   0% /sys/fs/cgroup
tmpfs           3.8G     0  3.8G   0% /var/lib/kubelet/pods/dfe8eed0-a72e-4747-b31a-af4cc9899260/volumes/kubernetes.io~secret/kubernetes-dashboard-certs
tmpfs           3.8G   12K  3.8G   1% /var/lib/kubelet/pods/dfe8eed0-a72e-4747-b31a-af4cc9899260/volumes/kubernetes.io~projected/kube-api-access-djrcg
tmpfs           3.8G   12K  3.8G   1% /var/lib/kubelet/pods/36406225-0046-401d-be75-d2cf7b73967f/volumes/kubernetes.io~projected/kube-api-access-q25km
tmpfs           3.8G   12K  3.8G   1% /var/lib/kubelet/pods/43cc79ae-72ba-4ad3-94db-c38ac9fdc571/volumes/kubernetes.io~projected/kube-api-access-6zf82
tmpfs           3.8G   12K  3.8G   1% /var/lib/kubelet/pods/39f65a88-c45b-4e9c-a4a0-3a117f900c82/volumes/kubernetes.io~projected/kube-api-access-7jlsf
shm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/6f5b03d7d8f6dc4fa128b4687261bd1b2ac7a731f4cb183c8b67d1721e6272db/shm
shm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/3bb0fb6fdb21f1e57e5584a1cf51e95b665cb9e3a5eba8d872f29c026b265b89/shm
overlay          30G  5.8G   23G  21% /run/containerd/io.containerd.runtime.v2.task/k8s.io/3bb0fb6fdb21f1e57e5584a1cf51e95b665cb9e3a5eba8d872f29c026b265b89/rootfs
overlay          30G  5.8G   23G  21% /run/containerd/io.containerd.runtime.v2.task/k8s.io/6f5b03d7d8f6dc4fa128b4687261bd1b2ac7a731f4cb183c8b67d1721e6272db/rootfs
overlay          30G  5.8G   23G  21% /run/containerd/io.containerd.runtime.v2.task/k8s.io/5a96adb99ef0da5f8ada80f7757a9f2decc7bb96b79f9858e28612a6cbb8d074/rootfs
overlay          30G  5.8G   23G  21% /run/containerd/io.containerd.runtime.v2.task/k8s.io/348d518fd7e3b93e2f5f6316ab33c89e7b849a3630b92f41b1cac9688bec644a/rootfs
shm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/67a2d095f05550d8256975e54a4ef42684c20c46c8968630d9464c786a366292/shm
overlay          30G  5.8G   23G  21% /run/containerd/io.containerd.runtime.v2.task/k8s.io/67a2d095f05550d8256975e54a4ef42684c20c46c8968630d9464c786a366292/rootfs
overlay          30G  5.8G   23G  21% /run/containerd/io.containerd.runtime.v2.task/k8s.io/0df8e96548b3db362bf91f3b3a96263eb1e54a6964ad54ea582dc86d11c19353/rootfs
shm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/68ce81c25a5208b453445dbda0e39cfde51e798b750cd3d0f0baca1dfadbe932/shm
overlay          30G  5.8G   23G  21% /run/containerd/io.containerd.runtime.v2.task/k8s.io/68ce81c25a5208b453445dbda0e39cfde51e798b750cd3d0f0baca1dfadbe932/rootfs
overlay          30G  5.8G   23G  21% /run/containerd/io.containerd.runtime.v2.task/k8s.io/d37edb7a4c2157ff56e3f6383a27c8f9a85bfa6385df85a7e3c4fd79094ab5db/rootfs
tmpfs           394M     0  394M   0% /run/user/0
tmpfs           3.8G   12K  3.8G   1% /var/lib/kubelet/pods/e9f817ae-ad96-4f2a-896e-6086d748a1e1/volumes/kubernetes.io~projected/kube-api-access-gf97w
/dev/sda         10G  125M   10G   1% /var/lib/kubelet/pods/e9f817ae-ad96-4f2a-896e-6086d748a1e1/volumes/kubernetes.io~iscsi/pv-10g
shm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/da51e42b752fad638f61f6c297b3a53590d544cbe299bb66c620cf063aa0c554/shm
overlay          30G  5.8G   23G  21% /run/containerd/io.containerd.runtime.v2.task/k8s.io/da51e42b752fad638f61f6c297b3a53590d544cbe299bb66c620cf063aa0c554/rootfs
overlay          30G  5.8G   23G  21% /run/containerd/io.containerd.runtime.v2.task/k8s.io/2933922e0487617d42b58daab68e6891f0215289a643b8f7291c5f2138114793/rootfs
root@k8s-nodew2:/# cd /var/lib/kubelet/pods/e9f817ae-ad96-4f2a-896e-6086d748a1e1/volumes/kubernetes.io~iscsi/pv-10g
root@k8s-nodew2:/var/lib/kubelet/pods/e9f817ae-ad96-4f2a-896e-6086d748a1e1/volumes/kubernetes.io~iscsi/pv-10g# ls
index.html  lost+found
```
Khi tạo pv liên kết với iSCSI thì thư mục disk PV tạo ra sẽ được mount vào mục iscsi cuả kubernetes.
## 4.4. Gỡ kết nối iSCSI khỏi node
1. Tắt kết nối iSCSI target:

`sudo iscsiadm -m node -T <target-name> -p <target-ip> -u`


Ví dụ:

`sudo iscsiadm -m node -T iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target -p 103.101.162.145 -u`


2. Xóa khai báo iSCSI target:

`sudo iscsiadm -m node -T <target-name> -p <target-ip> --op delete`


Ví dụ:

`sudo iscsiadm -m node -T iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target -p 103.101.162.145 --op delete`


3. Xóa thông tin iSCSI target được lưu trữ trong file /etc/iscsi/nodes:

`sudo rm -rf /etc/iscsi/nodes/<target-name>/<target-ip>,<port>/`


Ví dụ:

`sudo rm -rf /etc/iscsi/nodes/iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target/103.101.162.145,3260/`

## Fix lỗi không login được iscsi target

Error: 
```sh
root@k8s-nodew2:~# sudo iscsiadm -m node -T iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target -p 103.101.162.145 -l
Logging in to [iface: default, target: iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target, portal: 103.101.162.145,3260] (multiple)
iscsiadm: got read error (-1/104), daemon died?
iscsiadm: Could not login to [iface: default, target: iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target, portal: 103.101.162.145,3260].
iscsiadm: initiator reported error (18 - could not communicate to iscsid)
iscsiadm: Could not log into all portals
```

Lỗi `iscsiadm: got read error (-1/104)`, daemon died? thường xảy ra khi daemon service iscsid của hệ thống không hoạt động đúng cách, hoặc không được cấu hình đúng. Để khắc phục lỗi này, bạn có thể thực hiện các bước sau:  `journalctl -xe` show các lỗi đang xảy ra.

```sh
root@k8s-nodew2:~# journalctl -xe
Apr 27 19:24:41 k8s-nodew2 kubelet[800]: I0427 19:24:41.390435     800 kubelet_node_status.go:70] "Attempting to register node" node="k8s-nodew2"
Apr 27 19:24:41 k8s-nodew2 kubelet[800]: E0427 19:24:41.391660     800 kubelet_node_status.go:92] "Unable to register node with API server" err="Post \"https://45>
Apr 27 19:24:41 k8s-nodew2 sudo[27939]:     root : TTY=pts/0 ; PWD=/root ; USER=root ; COMMAND=/usr/bin/systemctl restart iscsid
Apr 27 19:24:41 k8s-nodew2 sudo[27939]: pam_unix(sudo:session): session opened for user root by root(uid=0)
Apr 27 19:24:41 k8s-nodew2 systemd[1]: Starting iSCSI initiator daemon (iscsid)...
-- Subject: A start job for unit iscsid.service has begun execution
-- Defined-By: systemd
-- Support: http://www.ubuntu.com/support
--
-- A start job for unit iscsid.service has begun execution.
--
-- The job identifier is 3185.
Apr 27 19:24:41 k8s-nodew2 startup-checks.sh[27942]: Error: /etc/iscsi/initiatorname.iscsi does not contain a valid InitiatorName.
Apr 27 19:24:41 k8s-nodew2 startup-checks.sh[27942]: The iSCSI driver has not been correctly installed and cannot start.
Apr 27 19:24:41 k8s-nodew2 systemd[1]: iscsid.service: Control process exited, code=exited, status=1/FAILURE
-- Subject: Unit process exited
-- Defined-By: systemd
-- Support: http://www.ubuntu.com/support
--
-- An ExecStartPre= process belonging to unit iscsid.service has exited.
--
-- The process' exit code is 'exited' and its exit status is 1.
Apr 27 19:24:41 k8s-nodew2 systemd[1]: iscsid.service: Failed with result 'exit-code'.
```
Lỗi do config iscsi initiatorname sai: `Apr 27 19:24:41 k8s-nodew2 startup-checks.sh[27942]: Error: /etc/iscsi/initiatorname.iscsi does not contain a valid InitiatorName`  
Thay thế `InitiatorName=iqn.1991-05.com.microsoft:k8s-iscsi-targe-target1-target` chính xác.  
Sau đó restart lại service `systemctl restart iscsid` sau đó thực hiện login lại.


# 5. Tổng kết
## 5.1. Các bước để sử dụng Persistent Volume với iSCSI trong Kubernetes

- Cấu hình iSCSI target trên máy chủ lưu trữ (storage server).

- Cài đặt iSCSI initiator trên các node trong Kubernetes cluster để kết nối đến iSCSI target.

- Tạo một Persistent Volume (PV) trong Kubernetes cluster để đại diện cho phần trữ lượng lưu trữ bên ngoài được cung cấp thông qua iSCSI. PV có thể được định nghĩa thông qua các thông số như địa chỉ IP của iSCSI target, tên iSCSI target, tên iSCSI phân vùng, dung lượng lưu trữ, và quyền truy cập.

- Tạo một Persistent Volume Claim (PVC) trong Kubernetes cluster để yêu cầu sử dụng một phần hoặc toàn bộ dung lượng của PV.

- Sử dụng PVC trong Pod bằng cách sử dụng đối tượng VolumeClaimTemplate trong file định nghĩa Pod. Khi Pod được tạo, Kubernetes sẽ tự động cung cấp một phiên bản của PV được yêu cầu thông qua PVC.

Khi một container được triển khai trên Kubernetes cluster, Kubernetes sẽ liên kết các PVC được sử dụng trong Pod với PV tương ứng để cung cấp truy cập đến các tài nguyên lưu trữ bên ngoài được cung cấp thông qua iSCSI. Khi container bị xóa hoặc di chuyển, Kubernetes sẽ tự động giải phóng các tài nguyên lưu trữ bên ngoài bằng cách giải phóng các PVC và PV được sử dụng trong Pod. 

## 5.2 Các thiết bị có thể cấu hình iSCSI
Đối với chi phí thấp có thể triển khai cài đặt OS window server hoặc linux đễ cài đặt iSCSI.
Đối với doanh nghiệp lớn có thể đầu tư các thiết bị lưu trữ như SAN (Storage Area Network) hoặc NAS (Network Attached Storage).





