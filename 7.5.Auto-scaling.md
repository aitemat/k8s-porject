# Auto Scaling
Kubernetes có tính năng tự động scale ứng dụng của bạn và quản lý chúng, được gọi là tự động scaling hoặc auto scaling. Tự động scaling giúp tận dụng tài nguyên của hệ thống hiệu quả hơn và đảm bảo ứng dụng của bạn đáp ứng nhu cầu của người dùng.  

Có hai loại tự động scaling trên Kubernetes:
Nói về scale thì có 2 cách scale là horizontal scaling và vertical scaling:  
- `Horizontal Pod Autoscaler (HPA)` là cách scale mà ta sẽ tăng số lượng worker (application) đang xử lý công việc hiện tại ra nhiều hơn. Ví dụ ta đang có 2 Pod để xử lý tích điểm cho client khi client tạo deal thành công, khi số lượng client tăng đột biến, 2 Pod hiện tại không thể xử lý kịp, ta sẽ scale số lượng Pod lên thành 4 Pod chẳng hạn (add và remove pods)    
- `Vertical Pod Autoscaler (VPA)` là cách scale thay vì tăng số lượng worker lên, ta sẽ tăng số lượng tài nguyên có thể sử dụng của ứng dụng đó lên, như là tăng số lượng cpu và memory của ứng dụng đó. Ví dụ ta có một model để train AI, thì việc train AI này ta không thể tách ra một model khác để tăng tốc độ train được, mà ta chỉ có thể tăng cpu và memory cho model đó. (tăng giảm CPU và memory pod).
- `Cluster Autoscaler (CA)`là một controller trong Kubernetes, có chức năng tự động điều chỉnh kích thước của các node trong cluster (cluster scaling) dựa trên nhu cầu thực tế của ứng dụng. Khi sử dụng CA, nếu các pod trong cluster không thể được lập lịch vào các node hiện có, CA sẽ tạo ra các node mới để đáp ứng nhu cầu, và tương tự nếu có quá nhiều node thì CA có thể thu hẹp lại kích thước của cluster bằng cách loại bỏ các node không cần thiết. Các hoạt động của CA được xác định dựa trên các thuật toán thông minh để đảm bảo hiệu quả và an toàn cho hệ thống. (add và remove node)

## 1. Horizontal Pod Autoscaler (HPA): tự động scale các Pod trong một Deployment, ReplicaSet, hoặc StatefulSet dựa trên CPU sử dụng, memory sử dụng, hoặc số lượng kết nối đến ứng dụng.
### HPA LÀ GÌ?
(HPA) là một tính năng của Kubernetes cho phép tự động điều chỉnh số lượng pods của một deployment, replica set hoặc stateful set dựa trên tải hoạt động của ứng dụng. HPA sử dụng các chỉ số như CPU sử dụng hoặc số lượng requests đang được xử lý để phát hiện tải hoạt động và điều chỉnh số lượng replica trong cluster.
### Cách hoạt động
Trong mọi cài đặt Kubernetes, theo mặc định, có hỗ trợ cho tài nguyên HPA và bộ điều khiển được liên kết.

Vòng điều khiển HPA liên tục giám sát chỉ số được định cấu hình, so sánh nó với giá trị mục tiêu của chỉ số đó, sau đó quyết định tăng hoặc giảm số lượng nhóm bản sao để đạt được giá trị mục tiêu.  

<img src="/images/HPA.jpg">

Sơ đồ cho thấy tài nguyên HPA hoạt động với tài nguyên triển khai và cập nhật tài nguyên đó dựa trên giá trị chỉ số mục tiêu. Bộ điều khiển nhóm ( Deployment ) sau đó sẽ tăng hoặc giảm số lượng nhóm bản sao đang chạy.

Nếu không có dự phòng, một vấn đề có thể xảy ra trong các tình huống này là sự cố. Đập mạnh là tình huống trong đó HPA thực hiện các hành động tự động thay đổi quy mô tiếp theo trước khi khối lượng công việc kết thúc phản hồi với các hành động tự động thay đổi quy mô trước đó. Vòng kiểm soát HPA tránh đập bằng cách chọn đề xuất số lượng nhóm lớn nhất trong năm phút qua.  
Phần này sẽ xem qua mã ví dụ cho biết cách HPA có thể được định cấu hình để tự động thay đổi quy mô nhóm ứng dụng dựa trên mức sử dụng CPU mục tiêu. Có hai cách để tạo tài nguyên HPA:

- Lệnh kubectl autoscale
- Tệp tài nguyên HPA YAML  
Đoạn mã này cho thấy việc tạo một triển khai Kubernetes và đối tượng HPA để tự động điều chỉnh quy mô các nhóm triển khai đó dựa trên tải CPU. Điều này được hiển thị từng bước cùng với nhận xét.

Tạo một không gian tên cho thử nghiệm HPA  
`kubectl create ns hpa-test`  
 namespace/hpa-test created  
Tạo triển khai để thử nghiệm HPA:  
`vi example-app.yaml`
```sh
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
  namespace: hpa-test
spec:
  selector:
    matchLabels:
      app: php-apache
  replicas: 1
  template:
    metadata:
      labels:
        app: php-apache
    spec:
      containers:
      - name: php-apache
        image: k8s.gcr.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
  namespace: hpa-test
  labels:
    app: php-apache
spec:
  ports:
  - port: 80
  selector:
    app: php-apache
``` 
 ` kubectl create -f example-app.yaml`
  deployment.apps/php-apache created
  service/php-apache created
 
Đảm bảo triển khai được tạo và nhóm đang chạy
```sh
# kubectl get deploy -n hpa-test
  NAME         READY   UP-TO-DATE   AVAILABLE   AGE
  php-apache   1/1     1            1           22s
```  
Sau khi triển khai và chạy, hãy tạo HPA bằng lệnh kubectl autoscale. HPA này sẽ duy trì tối thiểu 1 và tối đa 5 nhóm bản sao của quá trình triển khai để duy trì mức sử dụng CPU tổng thể ở mức 50%  
```sh
# kubectl -n hpa-test autoscale deployment php-apache --cpu-percent=50 --min=1 --max=5
  horizontalpodautoscaler.autoscaling/php-apache autoscaled
```  
Dạng khai báo của cùng một lệnh sẽ là tạo tài nguyên Kubernetes sau  
```sh
apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler
  metadata:
   name: php-apache
   namespace: hpa-test
  spec:
   scaleTargetRef:
     apiVersion: apps/v1
     kind: Deployment
     name: php-apache
   minReplicas: 1
   maxReplicas: 10
   targetCPUUtilizationPercentage: 50
```
Kiểm tra tình trạng hiện tại của HPA  
```sh
kubectl -n hpa-test get hpa
   
  NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
  php-apache   Deployment/php-apache   0%/50%    1         5         1          17s
```  
Hiện tại không có tải trên ứng dụng đang chạy nên các nhóm hiện tại và mong muốn bằng với số ban đầu là 1  
```sh
kubectl -n hpa-test get hpa php-apache -o yaml
   
  apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler
  metadata:
   name: php-apache
   namespace: hpa-test
   resourceVersion: "402396524"
   selfLink: /apis/autoscaling/v1/namespaces/hpa-test/horizontalpodautoscalers/php-apache
   uid: 6040eea9-0c2b-47de-9725-cfb78f17fe32
  spec:
   maxReplicas: 5
   minReplicas: 1
   scaleTargetRef:
     apiVersion: apps/v1
     kind: Deployment
     name: php-apache
   targetCPUUtilizationPercentage: 50
  status:
   currentCPUUtilizationPercentage: 0
   currentReplicas: 1
   desiredReplicas: 1
```
Bây giờ hãy chạy kiểm tra tải và xem lại trạng thái HPA  
`kubectl -n hpa-test run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"`  
Nếu bạn không thấy dấu nhắc lệnh, hãy thử nhấn enter.  
```sh
OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!
   
   
   
  kubectl -n hpa-test get hpa
   
  NAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
  php-apache   Deployment/php-apache   211%/50%   1         5         4          10m
```
Dừng tải bằng cách nhấn CTRL-C và sau đó nhận lại trạng thái hpa, điều này sẽ cho thấy mọi thứ trở lại bình thường và một bản sao đang chạy  
```sh
# kubectl -n hpa-test get hpa
   
  NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
  php-apache   Deployment/php-apache   0%/50%    1         5         1          20h
   
# kubectl -n hpa-test get hpa php-apache  -o yaml
  apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler
  metadata:
   name: php-apache
   namespace: hpa-test
   resourceVersion: "402402364"
   selfLink: /apis/autoscaling/v1/namespaces/hpa-test/horizontalpodautoscalers/php-apache
   uid: 6040eea9-0c2b-47de-9725-cfb78f17fe32
  spec:
   maxReplicas: 5
   minReplicas: 1
   scaleTargetRef:
     apiVersion: apps/v1
     kind: Deployment
     name: php-apache
   targetCPUUtilizationPercentage: 50
  status:
   currentCPUUtilizationPercentage: 0
   currentReplicas: 1
   desiredReplicas: 1
   lastScaleTime: "2021-07-04T08:22:54Z"
 ```
 Làm sạch tài nguyên  
 ```sh
 # kubectl delete ns hpa-test --cascade
  namespace "hpa-test" deleted
```  
### Các phương pháp hay nhất
- Để tận dụng các tính năng của Kubernetes như HPA, bạn cần phát triển ứng dụng có tính đến tỷ lệ ngang. Làm như vậy yêu cầu sử dụng kiến trúc microservice để thêm hỗ trợ riêng cho việc chạy các nhóm song song.
- Sử dụng tài nguyên HPA trên đối tượng Triển khai thay vì gắn trực tiếp nó vào bộ điều khiển Bản sao hoặc bộ điều khiển Bản sao.
- Sử dụng biểu mẫu khai báo để tạo tài nguyên HPA để chúng có thể được kiểm soát phiên bản. Cách tiếp cận này giúp theo dõi tốt hơn các thay đổi cấu hình theo thời gian.
- Đảm bảo xác định các yêu cầu tài nguyên cho nhóm khi sử dụng HPA. Các yêu cầu tài nguyên sẽ cho phép HPA đưa ra quyết định tối ưu khi thay đổi quy mô nhóm
### Các hạn chế 
- HPA không thể được sử dụng cùng với Vertical Pod Autoscaler dựa trên chỉ số CPU hoặc Bộ nhớ. VPA chỉ có thể thay đổi quy mô dựa trên các giá trị của CPU và bộ nhớ, vì vậy, khi VPA được bật, HPA phải sử dụng một hoặc nhiều số liệu tùy chỉnh để tránh xung đột về quy mô với VPA. Mỗi nhà cung cấp dịch vụ đám mây đều có bộ điều hợp chỉ số tùy chỉnh để cho phép HPA sử dụng chỉ số tùy chỉnh.
- HPA chỉ hoạt động đối với các ứng dụng không trạng thái hỗ trợ chạy song song nhiều phiên bản. Ngoài ra, HPA có thể được sử dụng với các bộ trạng thái dựa trên các nhóm bản sao. Đối với các ứng dụng không thể chạy dưới dạng nhiều nhóm, HPA không thể được sử dụng.
- HPA (và VPA) không xem xét IOPS, mạng và lưu trữ trong tính toán của họ, khiến các ứng dụng có nguy cơ bị chậm và ngừng hoạt động.
- HPA vẫn để lại cho quản trị viên gánh nặng xác định lãng phí trong cụm Kubernetes được tạo bởi các tài nguyên được yêu cầu dành riêng nhưng chưa được sử dụng ở cấp bộ chứa. Việc phát hiện việc sử dụng vùng chứa không hiệu quả không được Kubernetes giải quyết và yêu cầu công cụ của bên thứ ba được hỗ trợ bởi máy học.
## 2. Vertical Pod Autoscaler (VPA): tự động scale các Container trong một Pod dựa trên yêu cầu memory và CPU sử dụng của các Container.
 `Vertical Pod Autoscaler` (VPA) là một công cụ tự động scale các Container trong một Pod dựa trên yêu cầu sử dụng memory và CPU của các Container. Nó được thiết kế để giải quyết vấn đề về người quản trị hệ thống phải cấu hình lại tài nguyên để đáp ứng nhu cầu sử dụng của các ứng dụng.
### Cách hoạt động
Bộ điều khiển VPA quan sát việc sử dụng tài nguyên của một ứng dụng. Sau đó, bằng cách sử dụng thông tin sử dụng đó làm cơ sở, VPA đề xuất các giá trị giới hạn dưới, giới hạn trên và mục tiêu cho các yêu cầu tài nguyên cho các nhóm ứng dụng đó.

Nói một cách đơn giản, chúng ta có thể tóm tắt quy trình làm việc của VPA như sau:  
`Quan sát việc sử dụng tài nguyên` → `đề xuất yêu cầu tài nguyên` → `cập nhật tài nguyên`  
 
 <img src="/images/VPA.jpg">  
 
 Tùy thuộc vào cách bạn định cấu hình VPA, nó có thể:

Áp dụng trực tiếp các đề xuất bằng cách cập nhật/tạo lại các nhóm ( `updateMode = auto`).
Lưu trữ các giá trị được đề xuất để tham khảo ( `updateMode = off`).
Chỉ áp dụng các giá trị được đề xuất cho các nhóm mới tạo ( `updateMode = initial`).
Hãy nhớ rằng `updateMode = auto` bạn có thể sử dụng trong môi trường thử nghiệm hoặc dàn dựng nhưng không được sử dụng trong sản xuất. Lý do là nhóm khởi động lại khi VPA áp dụng thay đổi, điều này gây ra gián đoạn khối lượng công việc.

Chúng ta nên thiết lập `updateMode = off` trong sản xuất, cung cấp các đề xuất cho bảng điều khiển giám sát dung lượng, chẳng hạn như Grafana và áp dụng các đề xuất trong chu kỳ triển khai tiếp theo.
### Cách sử dụng VPA
Đây là một Triển khai Kubernetes mẫu sử dụng VPA cho các đề xuất tài nguyên.

Trước tiên, hãy tạo tài nguyên Triển khai bằng cách sử dụng tệp kê khai YAML sau được hiển thị bên dưới. Lưu ý rằng không có yêu cầu CPU hoặc bộ nhớ. Các nhóm trong Triển khai thuộc về VerticalPodAutoscaler(hiển thị trong đoạn tiếp theo) vì chúng được chỉ định bằng loại Deploymentvà tên, nginx-deployment.  
**Bước 1: Tạo deployment**  
```sh
apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: nginx-deployment
    labels:
      app: nginx
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: nginx
    template:
      metadata:
        labels:
          app: nginx
      spec:
        containers:
        - name: nginx
          image: nginx:1.7.8
          ports:
          - containerPort: 80
```  
**Bước 2:Tạo VPA**  
```sh
apiVersion: autoscaling.k8s.io/v1beta1
  kind: VerticalPodAutoscaler
  metadata:
    name: nginx-deployment-vpa
  spec:
    targetRef:
      apiVersion: "apps/v1"
      kind:       Deployment
      name:       nginx-deployment
    updatePolicy:
      updateMode: "Off"
```
Lưu ý rằng chế độ cập nhật được tắt. Thao tác này sẽ chỉ nhận các đề xuất chứ không tự động áp dụng chúng. Khi cấu hình được áp dụng, hãy nhận các đề xuất VPA bằng cách sử dụng `kubectl describe vpa nginx-deployment-vpa` lệnh.  
Các yêu cầu tài nguyên được đề xuất sẽ giống như sau:  
```sh
recommendation:
  containerRecommendations:
  - containerName: nginx
    lowerBound:
      cpu: 40m
      memory: 3100k
    target:
      cpu: 60m
      memory: 3500k
    upperBound:
      cpu: 831m
      memory: 8000k
```
Bạn có thể đặt UpdateMode thành auto trong ví dụ trên để cho phép tự động cập nhật các yêu cầu tài nguyên (giả sử rằng nó không được sử dụng trong môi trường sản xuất). Điều này sẽ khiến các nhóm được tạo lại với các giá trị mới cho các yêu cầu tài nguyên.  

## 3. Cluster Autoscaler (CA)
`Kubernetes Cluster Autoscaler` là một trong những công cụ tự động hóa phổ biến nhất để quản lý dung lượng phần cứng Kubernetes. Nó được hỗ trợ bởi các nền tảng đám mây chính và có thể hợp lý hóa quy trình quản lý cụm Kubernetes (K8s).  
### Tổng quan về Cluster Autoscaler
Khi các nhóm mới được triển khai và số lượng bản sao cho các nhóm hiện có tăng lên, các nút công nhân cụm có thể sử dụng hết tất cả các tài nguyên được phân bổ của chúng. Do đó, không thể lên lịch thêm nhóm nào cho các công nhân hiện có. Một số nhóm có thể chuyển sang trạng thái chờ xử lý, chờ CPU và bộ nhớ và có thể tạo ra sự cố ngừng hoạt động. Với tư cách là quản trị viên Kubernetes, bạn có thể giải quyết vấn đề này theo cách thủ công bằng cách thêm nhiều worker node hơn vào cụm để cho phép lập lịch cho các nhóm bổ sung.

Vấn đề là quy trình thủ công này tốn thời gian và quy mô kém. May mắn thay, Kubernetes Cluster Autoscaler có thể giải quyết vấn đề này bằng cách tự động hóa việc quản lý dung lượng. Cụ thể, Cluster Autoscaler tự động hóa quá trình thêm và xóa worker node khỏi cụm K8s.

Hầu hết các nhà cung cấp đám mây đều hỗ trợ Tự động thay đổi quy mô theo cụm, nhưng tính năng này không được hỗ trợ cho các môi trường K8s tự lưu trữ tại chỗ. Tự động thay đổi quy mô cụm là một tính năng “chỉ dành cho đám mây” vì các triển khai tại chỗ thiếu API để tạo và xóa máy ảo tự động cần thiết cho quy trình tự động thay đổi quy mô.

Theo mặc định, Cluster Autoscaler được cài đặt trên hầu hết các bản cài đặt đám mây của K8. Nếu Cluster Autoscaler chưa được cài đặt trong môi trường đám mây của bạn nhưng được hỗ trợ, thì bạn có thể cài đặt thủ công.  
### Yêu cầu Cluster autoscaler và nền tảng được hỗ trợ
Các nền tảng hỗ trợ autoscaler cluster trên Kubernetes bao gồm:

`Google Kubernetes Engine (GKE)`  
`Amazon Elastic Kubernetes Service (EKS)`  
`Microsoft Azure Kubernetes Service (AKS)`  
`DigitalOcean Kubernetes`  
`IBM Cloud Kubernetes Service (IKS)`  
`Red Hat OpenShift`  
`VMware Tanzu Kubernetes Grid (TKG)`  
Ngoài ra, các bản phân phối Kubernetes như Kubespray và Kops cũng hỗ trợ tính năng autoscaling cluster.  

### Hoạt động của Cluster Autoscaling
Bộ lập lịch Kubernetes tự động đặt các nhóm trên các nút worker bằng cách sử dụng chiến lược QoS nỗ lực nhất. Để Bộ tự động chia tỷ lệ cụm hoạt động như mong đợi và các ứng dụng nhận được tài nguyên máy chủ cơ bản mà chúng cần, các yêu cầu và giới hạn tài nguyên phải được xác định trên các nhóm. Nếu không có yêu cầu và giới hạn tài nguyên, Cluster Autoscaler không thể đưa ra quyết định chính xác.

Cluster AutoScaler kiểm tra định kỳ trạng thái của các nút và nhóm và thực hiện hành động dựa trên việc sử dụng nút hoặc trạng thái lập lịch nhóm. Khi Bộ tự động chia tỷ lệ cụm phát hiện các nhóm đang chờ xử lý trên cụm, nó sẽ thêm nhiều nút hơn cho đến khi các nhóm đang chờ xử lý được lên lịch hoặc cụm đạt đến giới hạn nút tối đa. Bộ tự động chia tỷ lệ cụm sẽ loại bỏ các nút bổ sung nếu mức sử dụng nút thấp và các nhóm có thể di chuyển đến các nút khác.  

<img src="/images/autoscaler.jpg">  

### Cách sử dụng

 
