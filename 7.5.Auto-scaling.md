# Auto Scaling
Kubernetes có tính năng tự động scale ứng dụng của bạn và quản lý chúng, được gọi là tự động scaling hoặc auto scaling. Tự động scaling giúp tận dụng tài nguyên của hệ thống hiệu quả hơn và đảm bảo ứng dụng của bạn đáp ứng nhu cầu của người dùng.  

Có hai loại tự động scaling trên Kubernetes:
Nói về scale thì có 2 cách scale là horizontal scaling và vertical scaling:  
`Horizontal scaling` là cách scale mà ta sẽ tăng số lượng worker (application) đang sử lý công việc hiện tại ra nhiều hơn. Ví dụ ta đang có 2 Pod để xử lý tích điểm cho client khi client tạo deal thành công, khi số lượng client tăng đột biến, 2 Pod hiện tại không thể xử lý kịp, ta sẽ scale số lượng Pod lên thành 4 Pod chẳng hạn.    
`Vertical scaling` là cách scale thay vì tăng số lượng worker lên, ta sẽ tăng số lượng tài nguyên có thể sử dụng của ứng dụng đó lên, như là tăng số lượng cpu và memory của ứng dụng đó. Ví dụ ta có một model để train AI, thì việc train AI này ta không thể tách ra một model khác để tăng tốc độ train được, mà ta chỉ có thể tăng cpu và memory cho model đó.
# Các khái niệm

## HPA LÀ GÌ?
HPA viết tắt từ Horizontal Pod Autoscaler – có thể hiểu đây là một bộ điều hành việc scale theo chiều ngang các Pod.

HPA đem lại các lợi ích: kinh tế, tự động hóa việc tăng giảm cấu hình hệ thống phù hợp với các hệ thống có khối lượng tải (mức enduser sủ dụng) biến đổi nhiều và khó dự đoán.

So với mô hình “truyền thống” kiểu fix cứng số lượng các pods, Auto scaling thích ứng để phù hợp với nhu cầu sử dụng. Chẳng hạn, khi lượng truy cập vào hệ thống vào buổi đêm giảm xuống, các pods có thể được set vào sleep mode (sleep mode để còn bật lại ngay đối phó với sự tăng bất thường từ lượng truy cập
### 1. Horizontal Pod Autoscaler (HPA): tự động scale các Pod trong một Deployment, ReplicaSet, hoặc StatefulSet dựa trên CPU sử dụng, memory sử dụng, hoặc số lượng kết nối đến ứng dụng.

Để sử dụng Horizontal Pod Autoscaler, bạn có thể thực hiện các bước sau:

1. Khai báo một tài nguyên HPA trong file YAML để chỉ định các thông số scale.

2. Cấu hình hệ thống để sử dụng một bộ điều khiển lên cương lưu để theo dõi và tính toán các thông số scale (ví dụ như Heapster).

3. Cài đặt metric server (nếu chưa cài đặt) để theo dõi các metric của các Pod.

4. Kích hoạt tự động scaling cho Pod với lệnh `kubectl autoscale deployment <tên deployment> --cpu-percent=<giá trị> --min=<tối thiểu> --max=<tối đa>`

Ví dụ:
`kubectl autoscale deployment nginx-example --cpu-percent=50 --min=2 --max=5`


Trong đó, giá trị cpu-percent là giá trị phần trăm của CPU sử dụng trung bình trên một pod, min và max là giới hạn tối thiểu và tối đa số lượng Pod. Khi CPU sử dụng trung bình trên các Pod vượt qua ngưỡng này, Kubernetes sẽ tự động tạo thêm các Pod mới để đáp ứng với nhu cầu tải.

Bạn có thể sử dụng lệnh kubectl get hpa để kiểm tra trạng thái của tự động scaling của deployment.

Sau đó, để test tự động scaling, bạn có thể tạo một áp lực tải trên các Pod để CPU sử dụng trung bình vượt qua ngưỡng. Có thể sử dụng công cụ hey để thực hiện điều này. Để cài đặt hey, bạn có thể chạy lệnh sau:

`go get -u github.com/rakyll/hey`


Sau khi cài đặt, bạn có thể sử dụng hey để tạo một áp lực tải trên các Pod như sau:

`hey -n 10000 -c 50 http://<IP or Hostname>`


Với lệnh trên, hey sẽ tạo 10.000 yêu cầu HTTP cho địa chỉ IP hoặc tên miền được chỉ định (<IP or Hostname>), với đường truyền tối đa là 50 yêu cầu đồng thời.

Sau khi tạo áp lực tải, chạy lệnh kubectl get hpa để xem số lượng các Pod đã được thêm vào để đáp ứng yêu cầu tải.

Nếu bạn muốn điều chỉnh cấu hình tự động scaling để phù hợp với nhu cầu của ứng dụng của mình, bạn có thể chỉnh sửa tài nguyên HPA hoặc sử dụng các giải pháp auto scaling khác như Cluster Autoscaler hay Vertical Pod Autoscaler.  
 
  
### 2. Vertical Pod Autoscaler (VPA): tự động scale các Container trong một Pod dựa trên yêu cầu memory và CPU sử dụng của các Container.
